{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9z3-HHjpnFm",
    "outputId": "c96ed876-63a5-454c-e70e-0d337994d55b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bumVwrt2pbMg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eLKjf7brDuV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWlWKrRKT33R"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QitiLjrapprM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/content/drive/MyDrive/case_study_1_var2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pzzJ4oU5uK7p"
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/fabiendaniel/elo-world\n",
    "#Function to load data into pandas and reduce memory usage\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKd0bLvauM5-"
   },
   "source": [
    "# Featurization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQgG1CBEp7hT"
   },
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/python-pandas-series-dt-date/\n",
    "def getFeaturesFromTrainAndTest(data):\n",
    "\n",
    "    min_dte = data['first_active_month'].dt.date.min()\n",
    "\n",
    "    #Time elapsed since first purchase\n",
    "    data['time_elapsed'] = (data['first_active_month'].dt.date - min_dte).dt.days\n",
    "\n",
    "    #Breaking first_active_month in year and month\n",
    "    data['month'] = data['first_active_month'].dt.month\n",
    "    data['year'] = data['first_active_month'].dt.year\n",
    "    data['day'] = data['first_active_month'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L556u0yfq3Vw"
   },
   "outputs": [],
   "source": [
    "def get_date_feature(df_m):\n",
    "\n",
    "  df = df_m.copy()\n",
    "  del df_m\n",
    "  df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "\n",
    "  df['m_op'] = df['purchase_date'].dt.month\n",
    "  df['y_op'] = df['purchase_date'].dt.year\n",
    "  \n",
    "  df['week_day'] = df['purchase_date'].dt.day.apply(lambda x : 0 if x > 4 else 1)\n",
    "  df['weekend'] = df['purchase_date'].dt.day.apply(lambda x : 0 if x < 4 else 1)\n",
    "\n",
    "  df['days'] = ((df['purchase_date'] - datetime.datetime(2011,11,1))).dt.days\n",
    "\n",
    "  df['month_diff'] = ((df['purchase_date'] - datetime.datetime(2011,11,1))).dt.days//30 #Month diff from the date of first card activation\n",
    "  \n",
    "  df['month_diff'] += df['month_lag']\n",
    "    \n",
    "  df = pd.get_dummies(df, columns =['m_op','y_op'])\n",
    " \n",
    "  agg_func_1 = {\n",
    "      'week_day' : ['sum','mean'],\n",
    "      'weekend' : ['sum','mean'],\n",
    "      'month_diff' : ['mean','min','max','std'],\n",
    "      'month_lag' : ['mean','min','max','std']     \n",
    "  }\n",
    "\n",
    "  agg_func_2 = { i : ['sum', 'mean'] for i in df.columns if ('m_op' in i) or ('y_op' in i)}\n",
    "  \n",
    "  df_1 = df.groupby('card_id').agg(agg_func_1)\n",
    "  df_1.columns = [ \"_\".join(col).strip() for col in df_1.columns]\n",
    "  df_1 = df_1.reset_index()\n",
    "\n",
    "  df_2 = df.groupby('card_id').agg(agg_func_2)\n",
    "  df_2.columns = [\"_\".join(col).strip() for col in  df_2.columns]\n",
    "  df_2 = df_2.reset_index()\n",
    "\n",
    "  df = df_1.merge(df_2, how = 'left', on = 'card_id')\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DV1m3Iq6rO27"
   },
   "outputs": [],
   "source": [
    "def get_purchase_features(df_main):\n",
    "  df = df_main.copy()\n",
    "  del df_main\n",
    "  agg_d = {'purchase_amount' : ['sum']}\n",
    "\n",
    "  df['purchase_date'] = pd.to_datetime(df['purchase_date'])\n",
    "\n",
    "  df['m_op'] = df['purchase_date'].dt.month\n",
    "  df['y_op'] = df['purchase_date'].dt.year\n",
    "\n",
    "  df_m = df.groupby(['card_id', 'm_op']).agg(agg_d)\n",
    "\n",
    "  #fm = pd.get_dummies(nm, columns = ['m_op'])\n",
    "  df_m.columns = [\"_\".join(col).strip() for col in df_m.columns]\n",
    "  df_m = df_m.reset_index()\n",
    "  #print('Pass')\n",
    "  df_m = pd.get_dummies(df_m, columns = ['m_op'])\n",
    "\n",
    "  ar = np.array(df_m.drop(columns = ['card_id','purchase_amount_sum']))\n",
    "\n",
    "  ar1 = np.array(df_m['purchase_amount_sum'])\n",
    "  ar1 = ar1.reshape(-1,1)\n",
    "\n",
    "  l1 = ['m_op_'+ str(i) for i in range(1,13)]\n",
    "\n",
    "  df_m[l1] = ar1*ar\n",
    "  df_m = df_m.drop(columns = ['purchase_amount_sum'])\n",
    "  df_m = df_m.groupby('card_id').agg(['sum','mean'])\n",
    "\n",
    "  #print('Pass')\n",
    "  ######################################################\n",
    "\n",
    "  df_y = df.groupby(['card_id', 'y_op']).agg(agg_d)\n",
    "\n",
    "  #fm = pd.get_dummies(nm, columns = ['m_op'])\n",
    "  df_y.columns = [\"_\".join(col).strip() for col in df_y.columns]\n",
    "  df_y = df_y.reset_index()\n",
    "\n",
    "  df_y = pd.get_dummies(df_y, columns = ['y_op'])\n",
    "\n",
    "  ar = np.array(df_y.drop(columns = ['card_id','purchase_amount_sum']))\n",
    "\n",
    "  ar1 = np.array(df_y['purchase_amount_sum'])\n",
    "  ar1 = ar1.reshape(-1,1)\n",
    "\n",
    "  l1 = ['y_op_2017','y_op_2018']\n",
    "\n",
    "  df_y[l1] = ar1*ar\n",
    "  df_y = df_y.drop(columns = ['purchase_amount_sum'])\n",
    "  df_y = df_y.groupby('card_id').agg(['sum','mean'])\n",
    "\n",
    "  df = df_y.merge(df_m, on = 'card_id', how = 'left')\n",
    "\n",
    "  df.columns = [\"_\".join(col).strip() for col in df.columns]\n",
    "  df.reset_index(inplace = True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKO6RMFTtle4"
   },
   "outputs": [],
   "source": [
    "def agg_merchant(df):\n",
    "\n",
    "  \n",
    "  dic_agg = {i:['mean'] for i in df.columns if (i not in ['merchant_id','category_4'])}\n",
    "  dic_agg['most_recent_sales_range'] = ['nunique']\n",
    "  dic_agg['most_recent_purchases_range'] = ['nunique']\n",
    "  dic_agg['merchant_group_id'] = ['nunique']\n",
    "  dic_agg['category_4'] = ['nunique','mean']\n",
    "\n",
    "\n",
    "  agg = df.groupby(['merchant_id']).agg(dic_agg)\n",
    "  \n",
    "  agg.columns = ['_'.join(col).strip() for col in agg.columns.values]\n",
    "  agg.reset_index(inplace=True)\n",
    "\n",
    "  return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RYkmPNUWuAwG"
   },
   "outputs": [],
   "source": [
    "def aggregate_transactions(df):\n",
    "\n",
    "    agg_func = {\n",
    "    'category_1': ['sum', 'mean'],\n",
    "\n",
    "    'category_3_0.0': ['mean','sum'],\n",
    "    'category_3_1.0': ['mean','sum'],\n",
    "    'category_3_2.0': ['mean','sum'],\n",
    "\n",
    "    'category_2_1.0': ['mean','sum'],\n",
    "    'category_2_2.0': ['mean','sum'],\n",
    "    'category_2_3.0': ['mean','sum'],\n",
    "    'category_2_4.0': ['mean','sum'],\n",
    "    'category_2_5.0': ['mean','sum'],\n",
    "\n",
    "    'authorized_flag_0' :['mean','sum'],\n",
    "    'authorized_flag_1' :['mean','sum'],\n",
    "\n",
    "    'merchant_id': ['nunique'],\n",
    "    'merchant_category_id': ['nunique',mode],\n",
    "\n",
    "    'state_id': ['nunique',mode],\n",
    "    'city_id': ['nunique',mode],\n",
    "    'subsector_id': ['nunique',mode],\n",
    "\n",
    "    'purchase_amount': ['sum', 'mean', 'max', 'min', std],\n",
    "    'installments': ['sum', 'mean', 'max', 'min', std],  \n",
    "    \n",
    "    'numerical_1_mean' : ['sum', 'mean', 'max', 'min',std],\n",
    "    'numerical_2_mean' : ['sum', 'mean', 'max', 'min',std],\n",
    "    \n",
    "\n",
    "    'merchant_group_id_nunique' : ['mean','sum'],\n",
    "    'most_recent_sales_range_nunique' : ['mean','sum'],\n",
    "    'most_recent_purchases_range_nunique' : ['mean','sum'],\n",
    "\n",
    "\n",
    "    'avg_sales_lag3_mean' : ['sum', 'mean',std],\n",
    "    'avg_purchases_lag3_mean':  ['sum', 'mean',std],\n",
    "    'active_months_lag3_mean':['sum', 'mean',std],\n",
    "\n",
    "    'avg_sales_lag6_mean':['sum', 'mean',std],\n",
    "    'avg_purchases_lag6_mean':['sum', 'mean',std],\n",
    "    'active_months_lag6_mean': ['sum', 'mean',std],\n",
    "\n",
    "    'avg_sales_lag12_mean':['sum', 'mean', std],\n",
    "    'avg_purchases_lag12_mean': ['sum', 'mean', std],\n",
    "    'active_months_lag12_mean': ['sum', 'mean',std],\n",
    "\n",
    "    'category_4_nunique' : ['sum','mean'],\n",
    "    'category_4_mean' : ['sum', 'mean']\n",
    "\n",
    "     }\n",
    "    \n",
    "    df_n = df.groupby('card_id').agg(agg_func)\n",
    "    \n",
    "    df_tr = (df.groupby('card_id')\\\n",
    "            .size()\\\n",
    "            .reset_index(name='transactions_count'))\n",
    "\n",
    "    df_n.columns = ['_'.join(col).strip() for col in df_n.columns.values]\n",
    "\n",
    "    df_n = df_n.merge(df_tr, how = 'left', on = 'card_id')\n",
    "\n",
    "    \n",
    "\n",
    "    return df_n   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3jtnh9zuHX_"
   },
   "outputs": [],
   "source": [
    "\n",
    "def aggregate_per_month(df):\n",
    "    grouped = df.groupby(['card_id', 'month_lag'])\n",
    "\n",
    "    agg_func = {\n",
    "            'purchase_amount': ['count', 'sum', 'mean', 'min', 'max',std],\n",
    "            'installments': ['count', 'sum', 'mean', 'min', 'max',std],\n",
    "            }\n",
    "\n",
    "    intermediate_group = grouped.agg(agg_func)\n",
    "    intermediate_group.columns = ['_'.join(col).strip() for col in intermediate_group.columns.values]\n",
    "    intermediate_group.reset_index(inplace=True)\n",
    "\n",
    "    final_group = intermediate_group.groupby('card_id').agg(['mean', 'std'])\n",
    "    final_group.columns = ['_'.join(col).strip() for col in final_group.columns.values]\n",
    "    final_group.reset_index(inplace=True)\n",
    "    \n",
    "    return final_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8EA1iTxNG7z"
   },
   "outputs": [],
   "source": [
    "import statistics as st\n",
    "from scipy import stats as s\n",
    "\n",
    "def std(x):\n",
    "  return np.std(x)\n",
    "\n",
    "def mode(x):\n",
    "  return int(s.mode(x)[0])\n",
    "def mode_c(x):\n",
    "  return x.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhHxUPHXNLD1"
   },
   "outputs": [],
   "source": [
    "def agg_merchant(df):\n",
    "\n",
    "  \n",
    "  dic_agg = {i:['mean'] for i in df.columns if (i not in ['merchant_id','category_4'])}\n",
    "  dic_agg['most_recent_sales_range'] = ['nunique']\n",
    "  dic_agg['most_recent_purchases_range'] = ['nunique']\n",
    "  dic_agg['merchant_group_id'] = ['nunique']\n",
    "  dic_agg['category_4'] = ['nunique','mean']\n",
    "\n",
    "\n",
    "  agg = df.groupby(['merchant_id']).agg(dic_agg)\n",
    "  \n",
    "  agg.columns = ['_'.join(col).strip() for col in agg.columns.values]\n",
    "  agg.reset_index(inplace=True)\n",
    "\n",
    "  return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILq7ZohauH3U"
   },
   "outputs": [],
   "source": [
    "def final_table():\n",
    "\n",
    "  #Importing Datasets\n",
    "\n",
    "  train_table  = reduce_mem_usage(pd.read_csv('train.csv',parse_dates = [\"first_active_month\"]))\n",
    "\n",
    "  '''test_table  = reduce_mem_usage(pd.read_csv('test.csv',parse_dates = [\"first_active_month\"]))\n",
    "  test_table = test_table.fillna(test_table.first_active_month[17])'''\n",
    "\n",
    "  historical_transaction = reduce_mem_usage(pd.read_csv('historical_transactions.csv'))\n",
    "  new_merchants_trasaction = reduce_mem_usage(pd.read_csv(\"new_merchant_transactions.csv\"))\n",
    "\n",
    "  merchants = reduce_mem_usage(pd.read_csv(\"/content/merchants.csv\"))\n",
    "\n",
    "\n",
    "  #Imputation\n",
    "  new_merchants_trasaction.category_1 = new_merchants_trasaction.category_1.map({'Y':1, 'N':0})\n",
    "  new_merchants_trasaction.category_3 = new_merchants_trasaction.category_3.map({'A':0, 'B':1, 'C':2})\n",
    "  new_merchants_trasaction.authorized_flag = new_merchants_trasaction.authorized_flag.map({'Y':1, 'N':0})\n",
    "\n",
    "  historical_transaction.category_1 = historical_transaction.category_1.map({'Y':1, 'N':0})\n",
    "  historical_transaction.category_3 = historical_transaction.category_3.map({'A':0, 'B':1, 'C':2})\n",
    "  historical_transaction.authorized_flag = historical_transaction.authorized_flag.map({'Y':1, 'N':0})\n",
    "\n",
    "  missing_id = historical_transaction.merchant_category_id.loc[historical_transaction.merchant_id.isnull()].unique()\n",
    "\n",
    "  missing_id_nm = new_merchants_trasaction.merchant_category_id.loc[new_merchants_trasaction.merchant_id.isnull()].unique()\n",
    "\n",
    "  \n",
    "  for i in tqdm(missing_id):\n",
    "      \n",
    "      mask = historical_transaction.merchant_category_id == i\n",
    "\n",
    "      value = historical_transaction.loc[mask,\"merchant_id\"].value_counts().index[0]\n",
    "      \n",
    "      historical_transaction.loc[mask,\"merchant_id\"] = historical_transaction_imp.loc[mask,\"merchant_id\"].fillna(value)\n",
    "\n",
    "  #imputation Merchant ids in new_merchant\n",
    "\n",
    "  for i in tqdm(missing_id_nm):\n",
    "      \n",
    "      mask = new_merchants_trasaction.merchant_category_id == i\n",
    "      value = new_merchants_trasaction.loc[mask,\"merchant_id\"].value_counts().index[0]\n",
    "      \n",
    "      #new_merchants_trasaction.merchant_id.loc[new_merchants_trasaction.merchant_category_id == i].fillna(value, inplace = True)\n",
    "      \n",
    "      new_merchants_trasaction.loc[mask,\"merchant_id\"] = new_merchants_trasaction.loc[mask,\"merchant_id\"].fillna(value)\n",
    "        \n",
    "\n",
    "  clean_nm = new_merchants_trasaction.drop(columns = ['card_id','purchase_date','merchant_id'])\n",
    "  clm = list(new_merchants_trasaction.drop(columns = ['card_id','purchase_date','merchant_id']).columns)\n",
    "  #print(\" Index numbers for category2 , 3 in new merchant\",clm.index('category_2'),clm.index('category_3'))\n",
    "\n",
    "  clean_ht = historical_transaction.drop(columns = ['card_id','purchase_date','merchant_id'])\n",
    "  clm = list(historical_transaction.drop(columns = ['card_id','purchase_date','merchant_id']).columns)\n",
    "  #print(\" Index numbers for category2 , 3 in historical data\",clm.index('category_2'),clm.index('category_3'))\n",
    "\n",
    "  imp = IterativeImputer(max_iter= 15, random_state=0)\n",
    "\n",
    "  clean_nm = np.round(imp.fit_transform(clean_nm))\n",
    "\n",
    "  clean_ht = np.round(imp.fit_transform(clean_ht))\n",
    "\n",
    "  new_merchants_trasaction['category_3'] = clean_nm[:,4]\n",
    "  new_merchants_trasaction['category_2'] = clean_nm[:,8]\n",
    "\n",
    "  historical_transaction['category_3'] = clean_ht[:,4]\n",
    "  historical_transaction['category_2'] = clean_ht[:,8]\n",
    "\n",
    "  #Rounding off\n",
    "  new_merchants_trasaction['category_3'] = new_merchants_trasaction['category_3'].apply(lambda x : 2 if x > 2 else x)\n",
    "  new_merchants_trasaction['category_2'] = new_merchants_trasaction['category_2'].apply(lambda x : 5 if x > 5 else x)\n",
    "\n",
    "  del clean_ht\n",
    "  del clean_nm\n",
    "\n",
    "  transaction = historical_transaction.append(new_merchants_trasaction, ignore_index=True)\n",
    "\n",
    "  del historical_transaction\n",
    "  del new_merchants_trasaction\n",
    "\n",
    "  transaction = pd.get_dummies(transaction, columns=['category_2', 'category_3','authorized_flag'])\n",
    "\n",
    "  #Merchant DATASET Imputation\n",
    "\n",
    "  merchants = merchants[['merchant_id','numerical_1', 'numerical_2','merchant_group_id',\n",
    "       'most_recent_sales_range', 'most_recent_purchases_range',\n",
    "       'avg_sales_lag3', 'avg_purchases_lag3', 'active_months_lag3',\n",
    "       'avg_sales_lag6', 'avg_purchases_lag6', 'active_months_lag6',\n",
    "       'avg_sales_lag12', 'avg_purchases_lag12', 'active_months_lag12',\n",
    "       'category_4']]\n",
    "\n",
    "  na_imputer = lambda x: x.fillna(x.mean())\n",
    "  inf_imputer = lambda x: x.replace([np.inf, -np.inf], x.value_counts().index[0])\n",
    "\n",
    "  Merchants[['avg_purchases_lag3']] = merchants[['avg_purchases_lag3']].apply(inf_imputer)\n",
    "  merchants[['avg_purchases_lag6']] = merchants[['avg_purchases_lag6']].apply(inf_imputer)\n",
    "  merchants[['avg_purchases_lag12']] = merchants[['avg_purchases_lag12']].apply(inf_imputer)\n",
    "\n",
    "  merchants[['avg_sales_lag3']] = merchants[['avg_sales_lag3']].apply(na_imputer)\n",
    "  merchants[['avg_sales_lag6']] = merchants[['avg_sales_lag6']].apply(na_imputer)\n",
    "  merchants[['avg_sales_lag12']] = merchants[['avg_sales_lag12']].apply(na_imputer)\n",
    "\n",
    "  merchants.category_4 = merchants.category_4.apply( lambda x : 1 if x == 'Y' else 0)\n",
    "\n",
    "  # Get Features\n",
    "\n",
    "  tr_purchase_f = get_purchase_features(transaction)\n",
    "  tr_date_f = get_date_feature(transaction)\n",
    "\n",
    "  merchants = agg_merchant(merchants)\n",
    "\n",
    "  transaction = transaction.merge(merchants, on = 'merchant_id', how = 'left')\n",
    "  del merchants\n",
    "\n",
    "  nm_tra_agg = aggregate_transactions(transaction)\n",
    "  nm_on_mlag = aggregate_per_month(transaction)\n",
    "\n",
    "  del transaction\n",
    "\n",
    "  #imputing na, inf values\n",
    "\n",
    "  nm_tra_agg = nm_tra_agg.replace([np.inf, - np.inf], np.nan)\n",
    "  list_na = [i for i in range(len(nm_tra_agg.columns)) if nm_tra_agg.isnull().any()[i] == True]\n",
    "  for i in list_na:\n",
    "    nm_tra_agg.iloc[:,i] = nm_tra_agg.iloc[:,i].fillna(nm_tra_agg.iloc[:,i].mean())\n",
    "\n",
    "  #Few more features\n",
    "  nm_tra_agg['purchase_importance_sum'] = nm_tra_agg['installments_sum']*np.log(abs(nm_tra_agg.transactions_count/nm_tra_agg.purchase_amount_sum))\n",
    "\n",
    "  nm_tra_agg['purchase_importance_mean'] = nm_tra_agg['installments_mean']*np.log(abs(nm_tra_agg.transactions_count/nm_tra_agg.purchase_amount_mean))\n",
    "\n",
    "  #nm_tra_agg['purchase_importance_std'] = nm_tra_agg['installments_std']*np.log(abs(nm_tra_agg.transactions_count/nm_tra_agg.purchase_amount_std))\n",
    "\n",
    "  nm_tra_agg['purchase_importance_min'] = nm_tra_agg['installments_min']*np.log(abs(nm_tra_agg.transactions_count/nm_tra_agg.purchase_amount_min))\n",
    "\n",
    "  nm_tra_agg['purchase_importance_max'] = nm_tra_agg['installments_mean']*np.log(abs(nm_tra_agg.transactions_count/nm_tra_agg.purchase_amount_max))\n",
    "\n",
    "\n",
    "\n",
    "  # Mean hot encoding of categorical values(f1, f2, f3)\n",
    "\n",
    "  train_table['rare_value'] = train_table['target'].apply(lambda x: 1 if x<= -30 else 0)\n",
    "\n",
    "  inf_imputer = lambda x: x.replace([np.inf, -np.inf], x.value_counts().index[0])\n",
    "\n",
    "  for i in ['feature_1', 'feature_2', 'feature_3']:\n",
    "\n",
    "      rare_data_mean = train_table.groupby([i])['rare_value'].mean()\n",
    "\n",
    "      train_table[i] = train_table[i].map(rare_data_mean)\n",
    "\n",
    "      test_table[i] = test_table[i].map(rare_data_mean)\n",
    "\n",
    "  \n",
    "\n",
    "  train_final = train_table.merge(nm_tra_agg, how = 'left', on = 'card_id')\n",
    "  #test_final = test_table.merge(nm_tra_agg, how = 'left', on = 'card_id')\n",
    "\n",
    "  del nm_tra_agg\n",
    "\n",
    "  train_final = train_final.merge(nm_on_mlag, how = 'left', on = 'card_id')\n",
    "  #test_final = test_final.merge(nm_on_mlag, how = 'left', on = 'card_id')\n",
    "\n",
    "  del nm_on_mlag\n",
    "\n",
    "  train_final = train_final.merge(tr_purchase_f, how = 'left', on = 'card_id')\n",
    "  #test_final = test_final.merge(tr_purchase_f, how = 'left', on = 'card_id')\n",
    "\n",
    "  del tr_purchase_f\n",
    "\n",
    "  train_final = train_final.merge(tr_date_f, how = 'left', on = 'card_id')\n",
    "  #test_final = test_final.merge(tr_date_f, how = 'left', on = 'card_id')\n",
    "\n",
    "  del tr_date_f\n",
    "\n",
    "  return train_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZu2fpZmv3Co"
   },
   "source": [
    "#Function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ZFzBKJ8v4f1"
   },
   "outputs": [],
   "source": [
    "def function_1 (data):\n",
    "\n",
    "  destination_1 = 'final_table.pkl'\n",
    "\n",
    "  if os.path.isfile(destination_1): \n",
    "    \n",
    "    final_table = reduce_mem_usage(pd.read_pickle('final_table.pkl'))\n",
    "\n",
    "  else:\n",
    "\n",
    "    try:\n",
    "\n",
    "      final_table = final_table()\n",
    "\n",
    "      pickle.dump((final_table), open('final_table.pkl','wb'))\n",
    "\n",
    "      final_table = reduce_mem_usage(pd.read_pickle('final_table.pkl'))\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Error:',e)\n",
    "\n",
    "  #importing best model\n",
    "\n",
    "  try :\n",
    "\n",
    "    model = pickle.load(open('kf_fold.pkl', 'rb'))\n",
    "\n",
    "  except Exception as e:\n",
    "          print('Error:',e)\n",
    "\n",
    "  id = data.card_id.values.tolist()\n",
    "\n",
    "  #print(\"pass\")\n",
    "  index = [final_table.loc[final_table.card_id == i].index.item() for i in id]\n",
    "\n",
    "  x = final_table.loc[index,:]\n",
    "\n",
    "  x = x.drop(columns = ['first_active_month','card_id','rare_value','target'])\n",
    "  \n",
    "  prediction = model.predict(x)\n",
    "\n",
    "  return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ys1pq0XYE81S"
   },
   "source": [
    "#Function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11sHO5bl3b9C"
   },
   "outputs": [],
   "source": [
    "def function_2 (data):\n",
    "\n",
    "  y_pred = function_1 (data)\n",
    "\n",
    "  target = data.target.values\n",
    "\n",
    "  rmse = mean_squared_error(data.target,y_pred)\n",
    "\n",
    "  print(\"Actual Loyalty Score:\", target)\n",
    "  print(\"\\n\\n Predicted Loyalty Score:\", y_pred)\n",
    "  print(\"\\n\\n Root mean squared error: \", rmse)\n",
    "\n",
    "  dic = {'rmse':rmse,'y_pred':y_pred}\n",
    "  return dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oj89AqZyrjve",
    "outputId": "b10a6598-fc18-4749-8295-6875aa1615e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to  4.04 Mb (56.2% reduction)\n"
     ]
    }
   ],
   "source": [
    "train_table  = reduce_mem_usage(pd.read_csv('/content/drive/MyDrive/case_study-1/train.csv',parse_dates = [\"first_active_month\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AANdUtihEtJV"
   },
   "source": [
    "# Final metric computed using function_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0YINWvap2te2",
    "outputId": "100ec1b5-4cae-43d3-fa25-76c8b04bbd4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 91.27 Mb (65.9% reduction)\n",
      "Actual Loyalty Score: [ 0.3928  0.688   0.1425 -0.1598  0.8716  0.2301  2.137  -0.0654  0.3   ]\n",
      "\n",
      "\n",
      " Predicted Loyalty Score: [-0.29901468 -0.1197515  -0.04907296 -0.19897493 -1.42109845 -0.17641045\n",
      " -0.6331256   0.26231499 -0.0065697 ]\n",
      "\n",
      "\n",
      " Root mean squared error:  1.6071562851272199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rmse': 1.6071562851272199,\n",
       " 'y_pred': array([-0.29901468, -0.1197515 , -0.04907296, -0.19897493, -1.42109845,\n",
       "        -0.17641045, -0.6331256 ,  0.26231499, -0.0065697 ])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_2(train_table[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "guYVc36xGtTE",
    "outputId": "463edf7f-50ce-44f6-c9ab-6f7ec88a7499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 91.27 Mb (65.9% reduction)\n",
      "Actual Loyalty Score: [-0.8203]\n",
      "\n",
      "\n",
      " Predicted Loyalty Score: [-0.26173569]\n",
      "\n",
      "\n",
      " Root mean squared error:  0.31200805818468286\n",
      "CPU times: user 997 ms, sys: 188 ms, total: 1.19 s\n",
      "Wall time: 1.35 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rmse': 0.31200805818468286, 'y_pred': array([-0.26173569])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "function_2(train_table[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHD677hxA04w"
   },
   "source": [
    "Deployment Video: https://youtu.be/8h3kMsrIgZU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAe1waQBA0Tn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final_Elo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
